{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Visualization for Strands Agents\n",
    "\n",
    "This notebook demonstrates how to visualize and analyze agent memory, providing insights into conversation patterns, memory usage, and engagement metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from collections import Counter\n",
    "\n",
    "# For visualization\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Directory where memory files are stored\n",
    "MEMORY_DIR = os.path.join(os.getcwd(), \"agent_memory\")\n",
    "os.makedirs(MEMORY_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_memory_file(file_path: str) -> Dict:\n",
    "    \"\"\"Load a memory file and return its contents as a dictionary.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return {}\n",
    "        \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def list_memory_files():\n",
    "    \"\"\"List all memory files in the memory directory.\"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(MEMORY_DIR):\n",
    "        if file.endswith('.json'):\n",
    "            files.append(os.path.join(MEMORY_DIR, file))\n",
    "    return files\n",
    "\n",
    "# List available memory files\n",
    "memory_files = list_memory_files()\n",
    "print(f\"Found {len(memory_files)} memory files:\")\n",
    "for file in memory_files:\n",
    "    print(f\"- {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Memory Analysis Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryAnalyzer:\n",
    "    \"\"\"Analyzes and visualizes agent memory.\"\"\"\n",
    "    \n",
    "    def __init__(self, memory_file: str):\n",
    "        self.memory_file = memory_file\n",
    "        self.memory_data = load_memory_file(memory_file)\n",
    "        self.sessions = self.memory_data.get('sessions', {})\n",
    "        self.summaries = self.memory_data.get('summaries', {})\n",
    "        self.tags = self.memory_data.get('tags', {})\n",
    "        \n",
    "    def get_session_stats(self):\n",
    "        \"\"\"Get basic statistics for each session.\"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for session_id, messages in self.sessions.items():\n",
    "            # Parse session timestamp\n",
    "            try:\n",
    "                # Assuming session_id format is YYYYMMDD_HHMMSS\n",
    "                timestamp = datetime.strptime(session_id, \"%Y%m%d_%H%M%S\")\n",
    "            except:\n",
    "                timestamp = None\n",
    "                \n",
    "            # Count messages by role\n",
    "            user_messages = sum(1 for msg in messages if msg.get('role') == 'user')\n",
    "            assistant_messages = sum(1 for msg in messages if msg.get('role') == 'assistant')\n",
    "            \n",
    "            # Calculate message lengths\n",
    "            user_msg_lengths = [len(msg.get('content', '')) for msg in messages if msg.get('role') == 'user']\n",
    "            assistant_msg_lengths = [len(msg.get('content', '')) for msg in messages if msg.get('role') == 'assistant']\n",
    "            \n",
    "            # Check for tool usage\n",
    "            tools_used = []\n",
    "            tool_count = 0\n",
    "            for msg in messages:\n",
    "                if msg.get('role') == 'assistant' and 'tool_calls' in msg:\n",
    "                    for tool_call in msg.get('tool_calls', []):\n",
    "                        tool_name = tool_call.get('name')\n",
    "                        if tool_name:\n",
    "                            tools_used.append(tool_name)\n",
    "                            tool_count += 1\n",
    "                            \n",
    "            # Get session summary if available\n",
    "            summary = self.summaries.get(session_id, '')\n",
    "            \n",
    "            stats.append({\n",
    "                'session_id': session_id,\n",
    "                'timestamp': timestamp,\n",
    "                'total_messages': len(messages),\n",
    "                'user_messages': user_messages,\n",
    "                'assistant_messages': assistant_messages,\n",
    "                'avg_user_msg_length': np.mean(user_msg_lengths) if user_msg_lengths else 0,\n",
    "                'avg_assistant_msg_length': np.mean(assistant_msg_lengths) if assistant_msg_lengths else 0,\n",
    "                'tools_used': Counter(tools_used),\n",
    "                'tool_count': tool_count,\n",
    "                'has_summary': bool(summary),\n",
    "                'summary': summary\n",
    "            })\n",
    "            \n",
    "        return stats\n",
    "    \n",
    "    def plot_session_activity(self):\n",
    "        \"\"\"Plot activity across different sessions.\"\"\"\n",
    "        stats = self.get_session_stats()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No sessions found to analyze.\")\n",
    "            return\n",
    "            \n",
    "        # Convert to DataFrame for easier plotting\n",
    "        df = pd.DataFrame(stats)\n",
    "        \n",
    "        # Sort by timestamp if available\n",
    "        if 'timestamp' in df.columns and not df['timestamp'].isna().all():\n",
    "            df = df.sort_values('timestamp')\n",
    "            session_labels = [str(ts.date()) for ts in df['timestamp'] if pd.notna(ts)]\n",
    "        else:\n",
    "            # Fall back to session_id for ordering\n",
    "            df = df.sort_values('session_id')\n",
    "            session_labels = df['session_id']\n",
    "            \n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('Session Activity Analysis', fontsize=16)\n",
    "        \n",
    "        # Plot 1: Message counts by session\n",
    "        ax = axes[0, 0]\n",
    "        ax.bar(range(len(df)), df['user_messages'], label='User')\n",
    "        ax.bar(range(len(df)), df['assistant_messages'], bottom=df['user_messages'], label='Assistant')\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(session_labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Number of Messages')\n",
    "        ax.set_title('Message Count by Session')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Plot 2: Message length by session\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(range(len(df)), df['avg_user_msg_length'], marker='o', label='User')\n",
    "        ax.plot(range(len(df)), df['avg_assistant_msg_length'], marker='s', label='Assistant')\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(session_labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Average Message Length (chars)')\n",
    "        ax.set_title('Average Message Length by Session')\n",
    "        ax.legend()\n",
    "        \n",
    "        # Plot 3: Tool usage count by session\n",
    "        ax = axes[1, 0]\n",
    "        ax.bar(range(len(df)), df['tool_count'])\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(session_labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Number of Tool Calls')\n",
    "        ax.set_title('Tool Usage by Session')\n",
    "        \n",
    "        # Plot 4: Message ratio (assistant/user ratio) by session\n",
    "        ax = axes[1, 1]\n",
    "        # Calculate ratio, handle division by zero\n",
    "        ratio = [a/u if u > 0 else 0 for a, u in zip(df['assistant_messages'], df['user_messages'])]\n",
    "        ax.bar(range(len(df)), ratio)\n",
    "        ax.axhline(y=1, color='r', linestyle='--', alpha=0.7)\n",
    "        ax.set_xticks(range(len(df)))\n",
    "        ax.set_xticklabels(session_labels, rotation=45, ha='right')\n",
    "        ax.set_ylabel('Assistant/User Message Ratio')\n",
    "        ax.set_title('Conversation Balance by Session')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_tool_usage(self):\n",
    "        \"\"\"Plot tool usage across all sessions.\"\"\"\n",
    "        stats = self.get_session_stats()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No sessions found to analyze.\")\n",
    "            return\n",
    "            \n",
    "        # Combine tool usage across all sessions\n",
    "        all_tools = Counter()\n",
    "        for session_stats in stats:\n",
    "            all_tools.update(session_stats['tools_used'])\n",
    "            \n",
    "        if not all_tools:\n",
    "            print(\"No tool usage found in any session.\")\n",
    "            return\n",
    "            \n",
    "        # Plot tool usage\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        tools = list(all_tools.keys())\n",
    "        counts = list(all_tools.values())\n",
    "        \n",
    "        # Sort by usage count\n",
    "        sorted_indices = np.argsort(counts)\n",
    "        tools = [tools[i] for i in sorted_indices]\n",
    "        counts = [counts[i] for i in sorted_indices]\n",
    "        \n",
    "        ax.barh(tools, counts)\n",
    "        ax.set_xlabel('Usage Count')\n",
    "        ax.set_title('Tool Usage Across All Sessions')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def word_cloud_from_messages(self, role='user'):\n",
    "        \"\"\"Generate a word cloud from messages of the specified role.\"\"\"\n",
    "        try:\n",
    "            from wordcloud import WordCloud\n",
    "            from nltk.corpus import stopwords\n",
    "            import nltk\n",
    "            \n",
    "            # Download stopwords if not already downloaded\n",
    "            try:\n",
    "                stopwords.words('english')\n",
    "            except:\n",
    "                nltk.download('stopwords')\n",
    "                \n",
    "            # Collect all messages from the specified role\n",
    "            all_text = \"\"\n",
    "            for session_id, messages in self.sessions.items():\n",
    "                for msg in messages:\n",
    "                    if msg.get('role') == role and 'content' in msg:\n",
    "                        all_text += msg['content'] + \" \"\n",
    "                        \n",
    "            if not all_text.strip():\n",
    "                print(f\"No {role} messages found.\")\n",
    "                return\n",
    "                \n",
    "            # Generate word cloud\n",
    "            stopwords_set = set(stopwords.words('english'))\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, height=400,\n",
    "                background_color='white',\n",
    "                stopwords=stopwords_set,\n",
    "                max_words=100\n",
    "            ).generate(all_text)\n",
    "            \n",
    "            # Display word cloud\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Word Cloud from {role.capitalize()} Messages\")\n",
    "            plt.show()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"Please install wordcloud and nltk to use this feature:\")\n",
    "            print(\"pip install wordcloud nltk\")\n",
    "            \n",
    "    def analyze_memory_growth(self):\n",
    "        \"\"\"Analyze memory size growth over time.\"\"\"\n",
    "        stats = self.get_session_stats()\n",
    "        \n",
    "        if not stats:\n",
    "            print(\"No sessions found to analyze.\")\n",
    "            return\n",
    "            \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(stats)\n",
    "        \n",
    "        # Sort by timestamp if available\n",
    "        if 'timestamp' in df.columns and not df['timestamp'].isna().all():\n",
    "            df = df.sort_values('timestamp')\n",
    "            time_labels = [str(ts.date()) for ts in df['timestamp'] if pd.notna(ts)]\n",
    "        else:\n",
    "            # Fall back to session_id for ordering\n",
    "            df = df.sort_values('session_id')\n",
    "            time_labels = df['session_id']\n",
    "            \n",
    "        # Calculate cumulative message counts\n",
    "        df['cumulative_messages'] = df['total_messages'].cumsum()\n",
    "        \n",
    "        # Calculate file size at each point\n",
    "        current_size = os.path.getsize(self.memory_file) / 1024  # KB\n",
    "        \n",
    "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot cumulative message count\n",
    "        ax1.plot(range(len(df)), df['cumulative_messages'], marker='o', color='blue')\n",
    "        ax1.set_xlabel('Session')\n",
    "        ax1.set_ylabel('Cumulative Message Count', color='blue')\n",
    "        ax1.tick_params(axis='y', labelcolor='blue')\n",
    "        ax1.set_xticks(range(len(df)))\n",
    "        ax1.set_xticklabels(time_labels, rotation=45, ha='right')\n",
    "        \n",
    "        # Add current file size annotation\n",
    "        ax1.text(0.02, 0.95, f'Current Memory Size: {current_size:.2f} KB', \n",
    "                transform=ax1.transAxes, fontsize=12,\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "        \n",
    "        ax1.set_title('Memory Growth Analysis')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print estimated growth rate\n",
    "        if len(df) > 1:\n",
    "            total_messages = df['total_messages'].sum()\n",
    "            bytes_per_message = (current_size * 1024) / total_messages if total_messages > 0 else 0\n",
    "            print(f\"Estimated memory usage per message: {bytes_per_message:.2f} bytes\")\n",
    "            print(f\"At this rate, 1000 messages would require approximately {(bytes_per_message * 1000) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Memory File\n",
    "\n",
    "Select a memory file to analyze. If you've run the previous example notebooks, you should have memory files available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we found memory files, analyze the first one\n",
    "# Otherwise, provide example data for demonstration\n",
    "\n",
    "if memory_files:\n",
    "    memory_file = memory_files[0]  # Choose the first memory file\n",
    "    print(f\"Analyzing memory file: {os.path.basename(memory_file)}\")\n",
    "    analyzer = MemoryAnalyzer(memory_file)\n",
    "else:\n",
    "    # Create a sample memory file for demonstration\n",
    "    print(\"No memory files found. Creating sample data for demonstration.\")\n",
    "    sample_file = os.path.join(MEMORY_DIR, \"sample_memory.json\")\n",
    "    \n",
    "    # Create sample data\n",
    "    sample_data = {\n",
    "        \"sessions\": {\n",
    "            \"20230601_120000\": [\n",
    "                {\"role\": \"user\", \"content\": \"Hello! My name is Alex and I'm interested in AI.\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"Hi Alex! I'd be happy to talk about AI with you. What aspects of AI interest you the most?\"}\n",
    "            ],\n",
    "            \"20230602_140000\": [\n",
    "                {\"role\": \"user\", \"content\": \"I'm working on a project involving natural language processing.\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"That's great! Natural language processing is a fascinating field. What kind of NLP project are you working on?\"}, \n",
    "                {\"role\": \"user\", \"content\": \"It's a sentiment analysis tool for customer reviews.\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"Sentiment analysis is very useful for understanding customer feedback. Are you using any specific libraries or models?\"}\n",
    "            ],\n",
    "            \"20230605_090000\": [\n",
    "                {\"role\": \"user\", \"content\": \"I need help with transformers for my NLP project.\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"I can help with transformers! They're powerful for NLP tasks. Do you have specific questions about using transformer models?\", \"tool_calls\": [{\"name\": \"search_documentation\"}]}, \n",
    "                {\"role\": \"user\", \"content\": \"Yes, I'm trying to fine-tune BERT for my sentiment analysis task.\"}, \n",
    "                {\"role\": \"assistant\", \"content\": \"Fine-tuning BERT for sentiment analysis is a good approach. Here are some steps to help you get started...\", \"tool_calls\": [{\"name\": \"code_example\"}]}\n",
    "            ]\n",
    "        },\n",
    "        \"summaries\": {\n",
    "            \"20230601_120000\": \"Initial introduction. User Alex is interested in AI.\",\n",
    "            \"20230602_140000\": \"Discussion about NLP project - sentiment analysis for customer reviews.\",\n",
    "            \"20230605_090000\": \"Help with transformer models, specifically fine-tuning BERT for sentiment analysis.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save sample data\n",
    "    with open(sample_file, 'w') as f:\n",
    "        json.dump(sample_data, f, indent=2)\n",
    "        \n",
    "    analyzer = MemoryAnalyzer(sample_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Memory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get session statistics\n",
    "session_stats = analyzer.get_session_stats()\n",
    "\n",
    "# Create a DataFrame for easier viewing\n",
    "stats_df = pd.DataFrame(session_stats)\n",
    "\n",
    "# Select columns to display\n",
    "display_columns = ['session_id', 'total_messages', 'user_messages', \n",
    "                   'assistant_messages', 'avg_user_msg_length', \n",
    "                   'avg_assistant_msg_length', 'tool_count', 'has_summary']\n",
    "\n",
    "# Display statistics\n",
    "stats_df[display_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot session activity\n",
    "analyzer.plot_session_activity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tool usage\n",
    "analyzer.plot_tool_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word cloud from user messages\n",
    "analyzer.word_cloud_from_messages(role='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze memory growth\n",
    "analyzer.analyze_memory_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Session Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show session summaries\n",
    "print(\"Session Summaries:\")\n",
    "for session_id, summary in analyzer.summaries.items():\n",
    "    print(f\"\\n{session_id}: {summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a sample of messages from each session\n",
    "for session_id, messages in analyzer.sessions.items():\n",
    "    print(f\"\\nSession: {session_id}\")\n",
    "    print(f\"Number of messages: {len(messages)}\")\n",
    "    \n",
    "    # Show up to 3 messages as example\n",
    "    sample_count = min(3, len(messages))\n",
    "    print(f\"Sample messages:\")\n",
    "    for i, msg in enumerate(messages[:sample_count]):\n",
    "        role = msg.get('role', 'unknown')\n",
    "        content = msg.get('content', '')\n",
    "        # Truncate long messages\n",
    "        if len(content) > 100:\n",
    "            content = content[:97] + \"...\"\n",
    "        print(f\"  {i+1}. {role.capitalize()}: {content}\")\n",
    "        \n",
    "    if len(messages) > sample_count:\n",
    "        print(f\"  ... and {len(messages) - sample_count} more messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Monitoring Tool\n",
    "\n",
    "Create a tool that can be used by Strands agents to monitor their own memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands.tools import tool\n",
    "\n",
    "@tool\n",
    "def monitor_memory(memory_file: str) -> str:\n",
    "    \"\"\"Monitor memory usage and provide statistics about the agent's memory.\"\"\"\n",
    "    if not os.path.exists(memory_file):\n",
    "        return f\"Memory file not found: {memory_file}\"\n",
    "        \n",
    "    try:\n",
    "        # Basic file stats\n",
    "        file_size = os.path.getsize(memory_file) / 1024  # KB\n",
    "        last_modified = datetime.fromtimestamp(os.path.getmtime(memory_file))\n",
    "        \n",
    "        # Load memory data\n",
    "        memory_data = load_memory_file(memory_file)\n",
    "        sessions = memory_data.get('sessions', {})\n",
    "        summaries = memory_data.get('summaries', {})\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_sessions = len(sessions)\n",
    "        total_messages = sum(len(messages) for messages in sessions.values())\n",
    "        user_messages = sum(sum(1 for msg in messages if msg.get('role') == 'user') \n",
    "                           for messages in sessions.values())\n",
    "        assistant_messages = sum(sum(1 for msg in messages if msg.get('role') == 'assistant') \n",
    "                               for messages in sessions.values())\n",
    "        summarized_sessions = sum(1 for session_id in sessions if session_id in summaries)\n",
    "        \n",
    "        # Get most recent session\n",
    "        if total_sessions > 0:\n",
    "            recent_session_id = sorted(sessions.keys())[-1]\n",
    "            recent_session_msgs = len(sessions[recent_session_id])\n",
    "        else:\n",
    "            recent_session_id = \"None\"\n",
    "            recent_session_msgs = 0\n",
    "        \n",
    "        # Format response\n",
    "        response = f\"\"\"Memory Usage Report:\n",
    "\n",
    "File: {os.path.basename(memory_file)}\n",
    "Size: {file_size:.2f} KB\n",
    "Last Modified: {last_modified}\n",
    "\n",
    "Sessions: {total_sessions}\n",
    "Total Messages: {total_messages}\n",
    "- User Messages: {user_messages}\n",
    "- Assistant Messages: {assistant_messages}\n",
    "Sessions with Summaries: {summarized_sessions}/{total_sessions}\n",
    "\n",
    "Most Recent Session: {recent_session_id}\n",
    "Messages in Recent Session: {recent_session_msgs}\n",
    "\n",
    "Estimated memory per message: {(file_size * 1024) / total_messages:.2f} bytes (if evenly distributed)\n",
    "        \"\"\"\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error analyzing memory file: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the memory monitoring tool\n",
    "if memory_files:\n",
    "    memory_stats = monitor_memory(memory_files[0])\n",
    "    print(memory_stats)\n",
    "else:\n",
    "    memory_stats = monitor_memory(os.path.join(MEMORY_DIR, \"sample_memory.json\"))\n",
    "    print(memory_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Memory Pruning and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(memory_file: str, strategy: str = 'summarize_old'):\n",
    "    \"\"\"Optimize memory usage by pruning or summarizing old conversations.\n",
    "    \n",
    "    Strategies:\n",
    "    - summarize_old: Replace older session details with summaries\n",
    "    - remove_oldest: Remove the oldest sessions\n",
    "    - compact: Remove unnecessary fields and truncate long messages\n",
    "    \"\"\"\n",
    "    if not os.path.exists(memory_file):\n",
    "        print(f\"Memory file not found: {memory_file}\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Load memory\n",
    "        memory_data = load_memory_file(memory_file)\n",
    "        sessions = memory_data.get('sessions', {})\n",
    "        summaries = memory_data.get('summaries', {})\n",
    "        \n",
    "        if not sessions:\n",
    "            print(\"No sessions found to optimize.\")\n",
    "            return\n",
    "            \n",
    "        # Backup original file\n",
    "        backup_file = memory_file + \".bak\"\n",
    "        with open(backup_file, 'w') as f:\n",
    "            json.dump(memory_data, f, indent=2)\n",
    "            \n",
    "        print(f\"Created backup at: {backup_file}\")\n",
    "        \n",
    "        # Sort sessions by date\n",
    "        sorted_sessions = sorted(sessions.keys())\n",
    "        \n",
    "        # Implement optimization strategies\n",
    "        if strategy == 'summarize_old':\n",
    "            # Keep the most recent 3 sessions, summarize the rest\n",
    "            keep_sessions = sorted_sessions[-3:] if len(sorted_sessions) > 3 else sorted_sessions\n",
    "            \n",
    "            for session_id in sorted_sessions:\n",
    "                if session_id not in keep_sessions:\n",
    "                    # If we don't have a summary yet, create a simple one\n",
    "                    if session_id not in summaries:\n",
    "                        messages = sessions[session_id]\n",
    "                        user_msgs = [msg['content'] for msg in messages if msg.get('role') == 'user']\n",
    "                        # Create a simple summary from the first user message\n",
    "                        if user_msgs:\n",
    "                            summaries[session_id] = f\"Session with {len(messages)} messages. First topic: {user_msgs[0][:100]}...\"\n",
    "                    \n",
    "                    # Replace full conversation with just first and last message\n",
    "                    if len(sessions[session_id]) > 2:\n",
    "                        sessions[session_id] = [sessions[session_id][0], sessions[session_id][-1]]\n",
    "            \n",
    "        elif strategy == 'remove_oldest':\n",
    "            # Keep only the most recent 5 sessions\n",
    "            keep_sessions = sorted_sessions[-5:] if len(sorted_sessions) > 5 else sorted_sessions\n",
    "            \n",
    "            # Remove old sessions\n",
    "            for session_id in list(sessions.keys()):\n",
    "                if session_id not in keep_sessions:\n",
    "                    del sessions[session_id]\n",
    "                    if session_id in summaries:\n",
    "                        del summaries[session_id]\n",
    "                        \n",
    "        elif strategy == 'compact':\n",
    "            # Compact all messages by removing unnecessary fields and truncating long content\n",
    "            for session_id, messages in sessions.items():\n",
    "                for i, msg in enumerate(messages):\n",
    "                    # Keep only essential fields\n",
    "                    essential = {}\n",
    "                    if 'role' in msg:\n",
    "                        essential['role'] = msg['role']\n",
    "                    if 'content' in msg:\n",
    "                        # Truncate very long messages\n",
    "                        content = msg['content']\n",
    "                        if len(content) > 500:\n",
    "                            essential['content'] = content[:497] + \"...\"\n",
    "                        else:\n",
    "                            essential['content'] = content\n",
    "                    # Add tool information if present, but simplified\n",
    "                    if 'tool_calls' in msg:\n",
    "                        essential['tool_calls'] = [{'name': tc.get('name')} for tc in msg['tool_calls']]\n",
    "                        \n",
    "                    sessions[session_id][i] = essential\n",
    "        \n",
    "        # Save optimized memory\n",
    "        memory_data['sessions'] = sessions\n",
    "        memory_data['summaries'] = summaries\n",
    "        \n",
    "        with open(memory_file, 'w') as f:\n",
    "            json.dump(memory_data, f, indent=2)\n",
    "            \n",
    "        # Report optimization results\n",
    "        original_size = os.path.getsize(backup_file) / 1024\n",
    "        new_size = os.path.getsize(memory_file) / 1024\n",
    "        saved = original_size - new_size\n",
    "        saved_percent = (saved / original_size) * 100 if original_size > 0 else 0\n",
    "        \n",
    "        print(f\"Memory optimization complete!\")\n",
    "        print(f\"Strategy used: {strategy}\")\n",
    "        print(f\"Original size: {original_size:.2f} KB\")\n",
    "        print(f\"New size: {new_size:.2f} KB\")\n",
    "        print(f\"Saved: {saved:.2f} KB ({saved_percent:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error optimizing memory: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization on a memory file\n",
    "if memory_files:\n",
    "    # Choose a strategy: 'summarize_old', 'remove_oldest', or 'compact'\n",
    "    optimize_memory(memory_files[0], strategy='summarize_old')\n",
    "else:\n",
    "    optimize_memory(os.path.join(MEMORY_DIR, \"sample_memory.json\"), strategy='compact')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Memory Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def search_memory_by_keyword(memory_file: str, keyword: str) -> str:\n",
    "    \"\"\"Search for specific keywords or topics across all memory sessions.\"\"\"\n",
    "    if not os.path.exists(memory_file):\n",
    "        return f\"Memory file not found: {memory_file}\"\n",
    "        \n",
    "    try:\n",
    "        memory_data = load_memory_file(memory_file)\n",
    "        sessions = memory_data.get('sessions', {})\n",
    "        \n",
    "        if not sessions:\n",
    "            return \"No sessions found to search.\"\n",
    "            \n",
    "        results = []\n",
    "        keyword = keyword.lower()\n",
    "        \n",
    "        for session_id, messages in sessions.items():\n",
    "            session_matches = []\n",
    "            \n",
    "            # Try to format the session date\n",
    "            try:\n",
    "                date_part = session_id.split('_')[0]\n",
    "                formatted_date = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:]}\"\n",
    "            except:\n",
    "                formatted_date = session_id\n",
    "                \n",
    "            for msg in messages:\n",
    "                content = msg.get('content', '').lower()\n",
    "                if keyword in content:\n",
    "                    role = \"User\" if msg.get('role') == 'user' else \"Assistant\"\n",
    "                    \n",
    "                    # Extract context around the keyword (snippet)\n",
    "                    keyword_pos = content.find(keyword)\n",
    "                    start_pos = max(0, keyword_pos - 40)\n",
    "                    end_pos = min(len(content), keyword_pos + len(keyword) + 40)\n",
    "                    \n",
    "                    snippet = content[start_pos:end_pos]\n",
    "                    if start_pos > 0:\n",
    "                        snippet = \"...\" + snippet\n",
    "                    if end_pos < len(content):\n",
    "                        snippet = snippet + \"...\"\n",
    "                        \n",
    "                    # Highlight the keyword\n",
    "                    highlighted = snippet.replace(keyword, f\"[{keyword}]\")\n",
    "                    \n",
    "                    session_matches.append(f\"{role}: {highlighted}\")\n",
    "            \n",
    "            if session_matches:\n",
    "                results.append(f\"\\nFrom session {formatted_date}:\")\n",
    "                results.extend(session_matches)\n",
    "                \n",
    "        if not results:\n",
    "            return f\"No matches found for '{keyword}'.\"\n",
    "            \n",
    "        return \"\\n\".join([f\"Search results for '{keyword}':\"]+results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error searching memory: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory search\n",
    "if memory_files:\n",
    "    search_results = search_memory_by_keyword(memory_files[0], \"project\")\n",
    "    print(search_results)\n",
    "else:\n",
    "    search_results = search_memory_by_keyword(os.path.join(MEMORY_DIR, \"sample_memory.json\"), \"project\")\n",
    "    print(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "This notebook demonstrated various techniques for visualizing and analyzing agent memory, including:\n",
    "\n",
    "1. **Session activity analysis** - Understanding conversation patterns across sessions\n",
    "2. **Tool usage visualization** - Identifying which tools are most frequently used\n",
    "3. **Content analysis** - Examining message content via word clouds and keyword searches\n",
    "4. **Memory optimization** - Techniques for reducing memory size while preserving key information\n",
    "5. **Memory monitoring** - Tools for agents to analyze their own memory usage\n",
    "\n",
    "These analytics and management capabilities are essential for maintaining efficient memory systems in production agent applications, especially as conversation histories grow over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}