{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca8e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Any, Iterable, Optional, TypedDict\n",
    "from typing_extensions import Unpack, override\n",
    "from strands.types.models import Model\n",
    "from strands.types.content import Messages, Role\n",
    "from strands.types.streaming import StreamEvent\n",
    "from strands.types.tools import ToolSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec481cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import Any, Iterable, Optional, TypedDict\n",
    "from typing_extensions import Unpack, override\n",
    "from strands.types.models import Model\n",
    "from strands.types.content import Messages, Role\n",
    "from strands.types.streaming import StreamEvent\n",
    "from strands.types.tools import ToolSpec\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings for self-signed certificates\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "class LLMGatewayModel(Model):\n",
    "    \"\"\"AI Gateway  model provider implementation.\"\"\"\n",
    "    \n",
    "    class ModelConfig(TypedDict):\n",
    "        model_id: str\n",
    "        max_tokens_to_sample: Optional[int]\n",
    "        temperature: Optional[float]\n",
    "        top_p: Optional[float] \n",
    "        top_k: Optional[int]\n",
    "        verify_ssl: Optional[bool]  # Added SSL verification option\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: str,\n",
    "        base_url: str = \"https://aigateway.com/v0/r1/model\",  ##Your API end point\n",
    "        **model_config: Unpack[ModelConfig]\n",
    "    ) -> None:\n",
    "        \n",
    "        # Set default model config\n",
    "        default_config = {\n",
    "            \"model_id\": \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "            \"max_tokens_to_sample\": 4096,\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": None,\n",
    "            \"verify_ssl\": False,  # Default to False for internal APIs\n",
    "            \"timeout\": 60,\n",
    "        }\n",
    "        default_config.update(model_config)\n",
    "        \n",
    "        self.config = LLMGatewayModel.ModelConfig(**default_config)\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    @override\n",
    "    def update_config(self, **model_config: Unpack[ModelConfig]) -> None:\n",
    "        self.config.update(model_config)\n",
    "\n",
    "    @override\n",
    "    def get_config(self) -> ModelConfig:\n",
    "        return self.config\n",
    "    \n",
    "    @override\n",
    "    def format_request(self, messages: Messages, tools: list[ToolSpec] | None = None, system_prompt: str | None = None) -> Any:\n",
    "\n",
    "        formatted_messages = []\n",
    "        \n",
    "        # Add system prompt as first message if provided\n",
    "        if system_prompt:\n",
    "            formatted_messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": system_prompt\n",
    "                    }\n",
    "                ]\n",
    "            })\n",
    "        \n",
    "        for message in messages:\n",
    "            # Handle both dict-style and object-style messages\n",
    "            if isinstance(message, dict):\n",
    "                role = message.get(\"role\")\n",
    "                content = message.get(\"content\")\n",
    "            else:\n",
    "                role = getattr(message, \"role\", None)\n",
    "                content = getattr(message, \"content\", None)\n",
    "            \n",
    "            # Format content as required by AI Gateway\n",
    "            if isinstance(content, str):\n",
    "                # Convert string content to the required format\n",
    "                formatted_content = [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": content\n",
    "                    }\n",
    "                ]\n",
    "            elif isinstance(content, list):\n",
    "                # Content is already in list format, keep as is\n",
    "                formatted_content = content\n",
    "            else:\n",
    "                # Fallback for other content types\n",
    "                formatted_content = [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": str(content)\n",
    "                    }\n",
    "                ]\n",
    "            \n",
    "            formatted_messages.append({\n",
    "                \"role\": role,\n",
    "                \"content\": formatted_content\n",
    "            })\n",
    "        \n",
    "        # Build request payload\n",
    "        request_payload = {\n",
    "            \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"Hello, how are you today?\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "            \"max_tokens\": self.config.get(\"max_tokens_to_sample\", 4096),\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"temperature\": self.config.get(\"temperature\", 0.7)\n",
    "        }\n",
    "        \n",
    "        # Add optional parameters if they exist\n",
    "        if self.config.get(\"top_p\") is not None:\n",
    "            request_payload[\"top_p\"] = self.config[\"top_p\"]\n",
    "        if self.config.get(\"top_k\") is not None:\n",
    "            request_payload[\"top_k\"] = self.config[\"top_k\"]\n",
    "            \n",
    "        # Add tools if provided\n",
    "        if tools:\n",
    "            request_payload[\"tools\"] = tools\n",
    "\n",
    "        print(f\"request_payload provided: {request_payload}\")\n",
    "            \n",
    "        return request_payload\n",
    "    \n",
    "    @override\n",
    "    def format_chunk(self, chunk: Any) -> StreamEvent:\n",
    "        if isinstance(chunk, dict):\n",
    "            chunk_type = chunk.get(\"type\")\n",
    "            \n",
    "            if chunk_type == \"message_start\":\n",
    "                return {\n",
    "                    \"type\": \"message_start\",\n",
    "                    \"message\": chunk.get(\"message\", {})\n",
    "                }\n",
    "            elif chunk_type == \"content_block_start\":\n",
    "                return {\n",
    "                    \"type\": \"content_block_start\",\n",
    "                    \"index\": chunk.get(\"index\", 0),\n",
    "                    \"content_block\": chunk.get(\"content_block\", {})\n",
    "                }\n",
    "            elif chunk_type == \"content_block_delta\":\n",
    "                return {\n",
    "                    \"type\": \"content_block_delta\",\n",
    "                    \"index\": chunk.get(\"index\", 0),\n",
    "                    \"delta\": chunk.get(\"delta\", {})\n",
    "                }\n",
    "            elif chunk_type == \"content_block_stop\":\n",
    "                return {\n",
    "                    \"type\": \"content_block_stop\",\n",
    "                    \"index\": chunk.get(\"index\", 0)\n",
    "                }\n",
    "            elif chunk_type == \"message_delta\":\n",
    "                return {\n",
    "                    \"type\": \"message_delta\",\n",
    "                    \"delta\": chunk.get(\"delta\", {})\n",
    "                }\n",
    "            elif chunk_type == \"message_stop\":\n",
    "                return {\n",
    "                    \"type\": \"message_stop\"\n",
    "                }\n",
    "            else:\n",
    "                # Handle unknown chunk types or text content\n",
    "                return {\n",
    "                    \"type\": \"content_block_delta\",\n",
    "                    \"index\": 0,\n",
    "                    \"delta\": {\n",
    "                        \"type\": \"text_delta\",\n",
    "                        \"text\": str(chunk)\n",
    "                    }\n",
    "                }\n",
    "        else:\n",
    "            # Handle string chunks\n",
    "            return {\n",
    "                \"type\": \"content_block_delta\",\n",
    "                \"index\": 0,\n",
    "                \"delta\": {\n",
    "                    \"type\": \"text_delta\",\n",
    "                    \"text\": str(chunk)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    @override\n",
    "    def stream(self, request: Any) -> Iterable[Any]:\n",
    "        # Build the complete URL\n",
    "        model_id = self.config[\"model_id\"]\n",
    "        url = f\"{self.base_url}/{model_id}/invoke\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "            # Removed Accept header that was causing the issue\n",
    "        }\n",
    "        \n",
    "        # SSL verification setting\n",
    "        verify_ssl = self.config.get(\"verify_ssl\", False)\n",
    "        \n",
    "        try:\n",
    "            # Make streaming request with SSL verification option\n",
    "            response = requests.post(\n",
    "                url,\n",
    "                headers=headers,\n",
    "                json=request,\n",
    "                stream=True,\n",
    "                timeout=self.config.get(\"timeout\", 60),\n",
    "                verify=verify_ssl  # This handles SSL certificate verification\n",
    "            )\n",
    "            print(f\"Request sent to {url} with headers: {headers}\")\n",
    "            print(f\"Request body: {json.dumps(request)}\")\n",
    "            print(f\"Response status code: {response.status_code}\")\n",
    "            print(f\"Response : {response}\")\n",
    "            print(f\"Response body: {response.text}\")\n",
    "            # Check for HTTP errors\n",
    "            response.raise_for_status()\n",
    "            \n",
    "        except requests.exceptions.SSLError as e:\n",
    "            print(f\"SSL Error: {e}\")\n",
    "            print(\"Tip: Try setting verify_ssl=False when creating the model if using internal/self-signed certificates\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request Error: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Yield message start event first\n",
    "        yield {\n",
    "            \"type\": \"message_start\",\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": []\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process streaming response\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line = line.decode('utf-8').strip()\n",
    "                \n",
    "                # Handle Server-Sent Events format\n",
    "                if line.startswith('data: '):\n",
    "                    data_part = line[6:]  # Remove 'data: ' prefix\n",
    "                    \n",
    "                    if data_part == '[DONE]':\n",
    "                        break\n",
    "                        \n",
    "                    try:\n",
    "                        event_data = json.loads(data_part)\n",
    "                        yield event_data\n",
    "                    except json.JSONDecodeError:\n",
    "                        # Handle non-JSON data\n",
    "                        yield {\n",
    "                            \"type\": \"content_block_delta\",\n",
    "                            \"index\": 0,\n",
    "                            \"delta\": {\n",
    "                                \"type\": \"text_delta\",\n",
    "                                \"text\": data_part\n",
    "                            }\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0181da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and factory function\n",
    "def create_llm_gateway_model(\n",
    "    api_key: str,\n",
    "    model_id: str = \"anthropic.claude-3-7-sonnet-20250219-v1:0-pgo\",\n",
    "    verify_ssl: bool = False,  # Default to False for internal APIs\n",
    "    **kwargs\n",
    ") -> LLMGatewayModel:\n",
    "    return LLMGatewayModel(\n",
    "        api_key=api_key,\n",
    "        model_id=model_id,\n",
    "        verify_ssl=verify_ssl,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582028ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the updated implementation\n",
    "from strands import Agent\n",
    "\n",
    "try:\n",
    "    # Initialize your custom model provider\n",
    "    gateway_model = create_llm_gateway_model(\n",
    "        api_key= ''# Using test key for now\n",
    "        model_id=\"anthropic.claude-3-7-sonnet-20250219-v1:0-pgo\",\n",
    "        max_tokens_to_sample=2000,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # Create a Strands agent using your model\n",
    "    agent = Agent(model=gateway_model)\n",
    "    \n",
    "    print(\"✅ LLMGatewayModel instance created successfully!\")\n",
    "    print(\"✅ Strands Agent created successfully!\")\n",
    "    \n",
    "    # Test the format_request method directly\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "    formatted_request = gateway_model.format_request(test_messages, tools=None, system_prompt=\"You are a helpful assistant.\")\n",
    "    print(\"✅ format_request method works correctly!\")\n",
    "    print(f\"Formatted request: {formatted_request}\")\n",
    "    \n",
    "    # Note: Actual agent call would require valid API key\n",
    "    print(\"\\n📝 Note: To test the full agent call, provide a valid AI Gateway API key.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d22c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Usage Example with SSL Fix and Debugging\n",
    "from strands import Agent\n",
    "import json\n",
    "\n",
    "try:\n",
    "    # Setting verify_ssl=False to handle self-signed certificates\n",
    "    gateway_model = create_llm_gateway_model(\n",
    "        api_key= '',\n",
    "        model_id=\"anthropic.claude-3-7-sonnet-20250219-v1:0-pgo\",\n",
    "        max_tokens_to_sample=2000,\n",
    "        temperature=0.7,\n",
    "        verify_ssl=False  # This fixes the SSL certificate verification error\n",
    "    )\n",
    "\n",
    "    print(\"✅ Model created successfully!\")\n",
    "    \n",
    "    # Test the request formatting first\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Hello, how are you today?\"}]\n",
    "    formatted_request = gateway_model.format_request(test_messages, tools=None, system_prompt=None)\n",
    "    print(f\"🔍 Formatted request: {json.dumps(formatted_request, indent=2)}\")\n",
    "    \n",
    "    # Create agent and use it\n",
    "    agent = Agent(model=gateway_model)\n",
    "    print(\"✅ Agent created successfully!\")\n",
    "    \n",
    "    response = agent(\"Hello, how are you today?\")\n",
    "    print(f\"🤖 Response: {response}\")\n",
    "    \n",
    "except requests.exceptions.HTTPError as e:\n",
    "    print(f\"❌ HTTP Error: {e}\")\n",
    "    if hasattr(e, 'response') and e.response is not None:\n",
    "        print(f\"📝 Response status: {e.response.status_code}\")\n",
    "        print(f\"📝 Response headers: {dict(e.response.headers)}\")\n",
    "        try:\n",
    "            error_body = e.response.text\n",
    "            print(f\"📝 Response body: {error_body}\")\n",
    "        except:\n",
    "            print(\"📝 Could not read response body\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
